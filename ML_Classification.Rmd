---
title: "Classification methods applied to an imbalanced big dataset"
author: 'Jorge Bueno Perez'
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r echo = F, results = 'hide', message= FALSE}
library(knitr)
library(rmdformats)
```

```{r setup, echo=FALSE, cache=FALSE}
## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
knitr::opts_chunk$set(echo = TRUE)
```

![Source: https://unsplash.com/photos/FkJ3aNGeFMY](/Users/lajobu/Desktop/Projects/ML/Classification/cars.jpeg)

# 1) `Project decription`:

This analysis is part of the final project of the subject `Machine Learning 1: classification methods` taught at the `University of Warsaw`. The final project consisted in two projects `regression` and `classification` made together with `Lashari Gochiashvili`. 

I would like to share with you the part I was taking care of, hopefully it will be helpful to someone.

The purpose of this analysis is to apply `classification` methods to a big dataset, in order to classify the cars with a `symboling` security level: `secure`, `neutral` and `risky`.

`Classification` method is a `machine supervised learning` technique which has the purpose of identtifying to which of a set categories a new observation belongs.

In this analysis several methods will be applied, such as `learning vector quantization`, `multinomial regression`, `penalized multinomial regression`, `k-nearest neighbors`, `support bvector machine`, `linear discriminant analysis` and `quadratic discriminant analysis`. 

Aditionaly, several techniches will be applied in order to find the best model `performance`, among which we can find `down-sampling`, `cross-validation`, `tuning` our model by different parameters and `pre processing`.

All of these techniques are based on the `caret package`, one of the most known `R package` for `Machine Learning`

# 2) `Data description`:

The dataset used in this analysis `cars`, can be found in `openml` website: https://www.openml.org/d/1398

`cars` is an artificial dataset generated by `BNG method` (Bayesian Network Generated), and it is based on the popular dataset with the same name that can be found in the `UCI Machine Learning Repository`:
https://archive.ics.uci.edu/ml/datasets/Automobile

* First we will load all the necesary `packages` and the `data`:

```{r echo = T, results = 'hide', message= FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(corrplot)
library(tibble)
library(nnet)
library(mlbench)
library(randomForest)
library(nnet)
library(stargazer)
library(DMwR)
library(party)
library(e1071)
library(kernlab)
library(scales)
library(class)
library(psych)
library(knitr)
library(expss)
library(reshape2)
library(pROC)
library(MASS)
```

```{r echo = F, results = 'hide', message= FALSE}
setwd('/Users/lajobu/Desktop/Projects/ML/Classification')
cars <- readRDS("cars.Rdata")
cars1 <- readRDS("cars1.Rdata")
```

```{r echo=FALSE, results='asis'}
paste("The dataset cars initialy has", dim(cars)[1], "rows and",  dim(cars)[2], "columns") %>% 
  knitr::knit_print()
```

## 2.1) `Features description`:

The dataset basically contains several `characteristics of cars`, an `insurance risk rating` and `normalized losses` in use as compared to other cars.

Below we cond find more details about the different `features`:

1. `normalized.losses`: numerical - It is the relative average loss payment per insured vehicle year, this variables was normalized
2. `make`: ordinal with 22 levels - Car brand 
  + "volkswagen", "volvo", "nissan", "porsche", "honda", "subaru", "mazda", "jaguar", "dodge", "mercury", "toyota", "chevrolet", "mercedes-benz", "peugot", "mitsubishi", "plymouth", "bmw", "saab", "isuzu", "renault", "alfa-romero" and "audi" 
3. `fuel.type`: ordinal with 2 levels - Type of fuel 
  + "diesel" and "gas"
4. `aspiration`: ordinal with 2 levels - Type of aspiration, it refers to breathin 
  + "turbo" and "std"
5. `num.of.doors`: ordinal with 2 levels - Number of door 
  +"four" and "two"
6. `body.style`: ordinal with 5 levels - Body style of the car (shape) 
  + "hatchback" "sedan" "wagon" "hardtop" and "convertible"
7. `drive.wheels`: ordinal with 3 levels - The kind of drive wheel
  + "fwd" "rwd" "4wd"
8. `engine.location`: ordinal with 2 levels - The location of the engine 
  + "front" "rear"
9. `wheel.base`: numerical - It is the distance between the centers of the front and rear wheels
10. `length`: numerical - The length of the car
11. `width`: numerical - The width of the car
12. `height`: numerical - The height of the car
13. `curb.weight`: numerical - The total mass of a vehicle with standard equipment and all necessary operating consumables
14. `engine.type`: ordinal with 7 levels - The kind of engine 
  + "ohc"   "ohcv"  "l"     "rotor" "ohcf"  "dohc"  "dohcv"
15. `num.of.cylinders`: ordinal with 7 levels - The number of cylinders
  + "four"   "eight"  "six"    "three"  "five"   "twelve" "two"
16. `engine.size`: numerical - The size of the engine
17. `fuel.system`: ordinal with 9 levels - The kind of fuel system 
  + "2bbl" "mfi"  "mpfi" "1bbl" "idi"  "spdi" "4bbl" "spfi"
18. `bore`: numerical - The diameter of each cylinder
19. `stroke`: numerical - The distance travelled by the piston in each cycle
20. `compression.ratio`: numerical - Measure based on the relative volumes of the combustion chamber and the cylinder
21. `horsepower`: numerical - The horse power of the engine
22. `peak.rpm`: numerical - Power band of the engine
23. `city.mpg`: numerical - It is the related distance traveled by a vehicle and the amount of fuel consumed in city
24. `highway.mpg`: numerical - It is the related distance traveled by a vehicle and the amount of fuel consumed in highway
25. `price`: numerical - Price of the car
26. `symboling`: ordinal with 7 levels - Indicates how safe is the car
    + -3: Highly secure car, -2: Moderate safe car, -1: Low safe car, 0: Neutral, not safe and not risky, +1 Low risky car, +2 Moderate risky car and +3 Highly risky car

For the purpose of this research, first of all, the target variable `symboling` will be transformed to three levels:

* `Secure`, corresponding with symboling levels -3, -2 and -1
* `Neutral`, neither safe nor risky car, corresponding with symboling levels 0
* `Risky`, corresponding with symboling levels +3, -+2 and +1

After will be transformed to `factor`.

```{r}
cars$symboling <-
  plyr::revalue(as.character(cars$symboling),
                c("-3" = "secure",
                  "-2" = "secure",
                  "-1" = "secure",
                  "0" = "neutral",
                  "1" = "risky",
                  "2" = "risky",
                  "3" = "risky")) %>%
  as.factor()
```

Below we cand find a barplot of the target variable `symboling`:

```{r echo=FALSE, message= FALSE, warming= FALSE}
barplot(
  table(cars[26]),
  col = "#9F1E42",
  las = 0,
  xlab = "Symboling",
  ylab = "",
  axes = FALSE
)
```

As we can see in the graph, the levels are `imbalanced`. There are much more cars with symboling level risky than secure, hence we should take further this into consideration, maybe applying some type of `resampling`.

## 2.2) `Numeric` variables:

There are `15 numeric` variables.

We can find below a `density plot` of these variables:

```{r echo = F, results = 'hide', message= FALSE, results='asis'}
cars_numeric_vars <-
  sapply(cars, is.numeric) %>%
  which() %>%
  names() 

cars_numeric_vars_index <- c()
  for (i in cars_numeric_vars) {
    cars_numeric_vars_index <-
      c(cars_numeric_vars_index, grep(i, colnames(cars)))
}
```

```{r echo=FALSE, message= FALSE, warming= FALSE}
par(mar=c(2,1,1,1), mfrow=c(3,5), cex= 0.6)
for(i in cars_numeric_vars_index) {
  d <- density(cars[,i])
  plot(d, main= names(cars[i]), xlab= '', ylab= '', yaxt='n')
  polygon(d, col="#9F1E42", border="black")
}
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE}
par(mfrow=c(1,1))
```

Below we can find the main `statistical moments` for the numeric variables: 

```{r echo=FALSE}
options(scipen= 999)
describe(dplyr::select_if(cars, is.numeric)) %>% 
kable(align = "l",digits = 2)
```

## 2.3) `Ordinal` variables:

There are `11 ordinal` variables, one of them is the target variable.

We can find below a barplot of this variables:

```{r echo=FALSE, message= FALSE, warming= FALSE, results='asis'}
cars_characteristics_vars <-
  sapply(cars1, is.factor) %>%
  which() %>%
  names() 
cars_characteristics_vars <- cars_characteristics_vars[-11]
```

```{r echo=FALSE, message= FALSE, warming= FALSE}
cars_char_index <- c()
  for (i in cars_characteristics_vars) {
    cars_char_index <-
      c(cars_char_index, grep(i, colnames(cars1)))
  }
par(mar=c(2,1,1,1), mfrow=c(2,5), cex= 0.6)
for(i in cars_char_index) {
  plot(cars1[,i], xlab= NULL, ylab= NULL,
       main = names(cars1[i]), col="#9F1E42")
}
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE}
par(mfrow=c(1,1))
```

Fist of all, we will create `crossvalidation tables` with the taget variable `symboling`, and the rest of the `ordinal variables`. We will be able to see in percentage how the different variables are distributed, along the target variable:

A) For `body.style` and `drive.wheels`:

```{r echo=FALSE}
cro_cpct(cars$symboling, list(total(), cars$body.style, cars$drive.wheels))
```

* Most total common cars:
  + For `body.style` - `sedan`
  + For `drive.wheels` - `fwd`
  
* Most secure and risky cars by features:
  + For `body.style` - `convertible` is the most secure car
  + For `body.style` - `hatchback` is the most risky car
  + For `drive.wheels` - `4wd` is the most secure car
  + For `drive.wheels` - `fwd` is the most risky car

B) For `fuel.type`, `aspiration`, `num.of.doors` and `engine.location`:

```{r echo=FALSE}
cro_cpct(cars$symboling, list(total(), cars$fuel.type, cars$aspiration, cars$num.of.doors, cars$engine.location))
```

* Most total common cars:
  + For `fuel.type` - `gas`
  + For `aspiration` - `std`
  + For `num.of.doors` - `four`
  + For `engine.location` - `front`
  
* Most secure and risky cars by features:
  + For `fuel.type` - `gas` is more likely to be risky or secure
  + For `fuel.type` - `diesel` is the most neutral car
  + For `aspiration` - `std` is the most secure car
  + For `aspiration` - `turbo` is the most risky car
  + For `num.of.doors` - `four` is the most secure car
  + For `num.of.doors` - `two` is the most risky car
  + For `engine.location` - `front` is more likely to be risky or secure
  + For `engine.location` - `rear` is the most neutral

C) For `engine.type`:

```{r echo=FALSE}
cro_cpct(cars$symboling, list(total(), cars$engine.type))
```

* Most total common cars:
  + For `engine.type` - `ohc`

* Most secure and risky cars by features:
  + For `engine.type` - `ohxf` is the most secure car
  + For `engine.type` - `dohc` is the most risky car

D) For `num.of.cylinders`:

```{r echo=FALSE}
cro_cpct(cars$symboling, list(total(), cars$num.of.cylinders))
```

* Most total common cars:
  + For `num.of.cylinders` - `four`

* Most secure and risky cars by features:
  + For `num.of.cylinders` - `six` is the most secure car
  + For `num.of.cylinders` - `three` is the most risky car

E) For `fuel.system`:

```{r echo=FALSE}
cro_cpct(cars$symboling, list(total(), cars$fuel.system))
```

* Most total common cars:
  + For `fuel.system` - `mpfi`

* Most secure and risky cars by features:
  + For `fuel.system` - `mpfi` is the most secure car
  + For `fuel.system` - `idi` is the most risky car

F) For `make`:

```{r echo=FALSE}
a <- c("volkswagen", "volvo", "nissan", "porsche", "honda", "subaru", "mazda", "jaguar", "dodge", "mercury", "toyota")
c <- cars %>% 
  filter(cars$make %in% a)
cro_cpct(c$symboling, list(total(), c$make))
b <- c("chevrolet", "mercedes-benz", "peugot", "mitsubishi", "plymouth", "bmw", "saab", "isuzu", "renault", "alfa-romero", "audi")
d <- cars %>% 
  filter(cars$make %in% b)
cro_cpct(d$symboling, list(total(), d$make))
```

* Most total common cars:
  + For `make` - `toyota`

* Most secure and risky cars by features:
  + For `make` - `volvo` is the most secure car
  + For `make` - `saab` is the most risky car

# 3) `Cleaning the data`:

Before applying any Machine Learning algorithm it is important to have the data clean. It can `improve the results` but also `reduce the computational time`. Even in some cases we will not be able to apply the algorithms without this step.

```{r echo = F, results = 'hide', message= FALSE}
cars_mult_bin_vars <-
  sapply(cars, is.character) %>%
  which() %>%
  names() 
cars_mult_bin_vars
```

## 3.1) `Var. transformation`:

### 3.1.1) `Encoding` and conversion to `factors`:

All the ordinal variables of the dataset `cars` are characteristic, except for the target variable `symboling`. All these variables should be transformed to factors, we will do that with the function `as.factor()`.

Additionaly, `integer encoding` will be applied, this type of `one-hot encoding` transforms the `characteristic` features to `numbers`, without loosing any information, or having any impact in the final results. In large datasets like this one this step is important, in order to `use less memory` when saving the files. Aaprt from that, it is important for some models which are only able to manage `numeric` variables, in case of saving these as `numeric`.

Firstly, the `characteristic variables` will be converted to `factors` for the models that are not sensible to data distribution, but after will be transformed to `numeric` for the models sensible to data distribution.

```{r echo = F,  message= FALSE}
cars$fuel.type <-
  plyr::revalue(cars$fuel.type,
                           c("diesel" = 0,
                             "gas" = 1)) %>% 
  as.factor()
cars$aspiration <-
  plyr::revalue(cars$aspiration,
                c("turbo" = 0,
                  "std" = 1)) %>%
  as.factor()
cars$num.of.doors <-
  plyr::revalue(cars$num.of.doors,
                c("two" = 0,
                  "four" = 1)) %>%
  as.factor()
cars$engine.location <-
  plyr::revalue(cars$engine.location,
                c("front" = 0,
                  "rear" = 1)) %>%
  as.factor()
cars$drive.wheels <-
  plyr::revalue(cars$drive.wheels,
                c(
                  "fwd" = 0,
                  "rwd" = 1,
                  "4wd" = 2
                )) %>%
  as.factor()
cars$body.style <-
  plyr::revalue(
    cars$body.style,
    c(
      "hatchback" = 0,
      "sedan" = 1,
      "wagon" = 2,
      "hardtop" = 3,
      "convertible" = 4
    )
  ) %>%
  as.factor()
cars$engine.type <-
  plyr::revalue(
    cars$engine.type,
    c(
      "ohc" = 0,
      "ohcv" = 1,
      "l" = 2,
      "rotor" = 3,
      "ohcf" = 4,
      "dohc" = 5,
      "dohcv" = 6
    )
  ) %>%
  as.factor()
cars$num.of.cylinders <-
  plyr::revalue(
    cars$num.of.cylinders,
    c(
      "two" = 0,
      "three" = 1,
      "four" = 2,
      "five" = 3,
      "six" = 4,
      "eight" = 5,
      "twelve" = 6
    )
  ) %>%
  as.factor()
cars$fuel.system <-
  plyr::revalue(
    cars$fuel.system,
    c(
      "mfi" = 0,
      "mpfi" = 1,
      "idi" = 2,
      "spdi" = 3,
      "spfi" = 4,
      "1bbl" = 5,
      "2bbl" = 6,
      "4bbl" = 8
    )
  ) %>%
  as.factor()
cars$make <-
  plyr::revalue(
    cars$make,
    c(
      "volkswagen" = 0, 
      "volvo" = 1, 
      "nissan" = 2, 
      "porsche" = 3, 
      "honda" = 4, 
      "subaru" = 5, 
      "mazda" = 6, 
      "jaguar" = 7, 
      "dodge" = 8, 
      "mercury" = 9, 
      "toyota" = 10, 
      "chevrolet" = 11, 
      "mercedes-benz" = 12, 
      "peugot" = 13, 
      "mitsubishi" = 14, 
      "plymouth" = 15, 
      "bmw" = 16, 
      "saab" = 17, 
      "isuzu" = 18, 
      "renault" = 19, 
      "alfa-romero" = 20, 
      "audi" = 21
    )
  ) %>%
  as.factor()
```

After applying `as.factor()`, we should check that there is not any `character`:

```{r message= FALSE, results='asis'}
any(sapply(cars, is.character)) %>% 
  knitr::knit_print()
```

It is `FALSE`, hence we can continue further.

### 3.1.2) `Scalling` the data:

There are many numeric variables with different scale, and this can be a problem for the algorithms that are based on `euclidean distance`.

`Caret package` gives the posibility to apply `preProcess` to scale the data when training the model. However, in our case we will scale the data before, because some algorithms are applied from other different packages.

The type of selected `scale` is `range`, it scales the numeric data in the interval [0, 1], but not the factors.

```{r message= FALSE}
cars_preProces <-
  preProcess(cars, method = c("range"))
cars <- predict(cars_preProces,
               cars)
```

## 3.2) `Missing values`:

```{r message= FALSE, results='asis'}
any(is.na(cars)) %>% 
  knitr::knit_print()
```

There is not any missing value, so we can continue further.

## 3.3) `Unique variables`:

Finally, we should check the `numeric variables`, in order to be sure tha they are unique.

To consider one variable as unique, a `threshold` of 500k was selected, hence half of the possible maximum number of unique values, `0.5 * 1.000.000 = 500.000`

In order to be able to check this, the function `find_if_unique_length` was created:

```{r message= FALSE, result= "hide"}
find_if_unique_length <- function(x) {
  cars_numeric_vars_index <- c()
  for (i in cars_numeric_vars) {
    cars_numeric_vars_index <-
      c(cars_numeric_vars_index, grep(i, colnames(x)))
  }
  cars_numeric_vars_unique <- c()
  for (i in cars_numeric_vars_index) {
    a <- length(unique(x[, i]))
    if (a < (0.5) * dim(cars)[1]) {
      cars_numeric_vars_unique <-
        c(cars_numeric_vars_unique,
          paste(colnames(cars[i]), "NOT unique"))
    }
    else {
      cars_numeric_vars_unique <-
        c(cars_numeric_vars_unique, paste(colnames(cars[i]), "UNIQUE"))
    }
  }
  return(cars_numeric_vars_unique)
}
```

Now we will pass the function to our dataset:

```{r message= FALSE}
data.frame(Results= find_if_unique_length(cars)) %>% 
 kable(align = "l",digits = 2) 
```

As per the selected threshold of 500k, we can conclude that all the variables are `unique`, so we are ready to divide our data in two parts.

# 4) `Data partitioning`:

## 4.1) For models that can manage ordinal variables:

We are ready to divide the data in two samples, `train` and `test`. The train sample is used to train the model, and the test sample is used to make the prediction and verify the performance of the model.

The data will be divided in `70% training`, and `30% test`, with the help of the function `createDataPartition` of the `caret package`:

```{r message= FALSE, eval=FALSE}
set.seed(16)
cars_which_train <-
  createDataPartition(cars$symboling,
                      p = 0.7, 
                      list = FALSE)
```

### 4.1.1) `Training` sample:

```{r message= FALSE, eval=FALSE}
cars_train <- cars[cars_which_train,]
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE, eval=FALSE}
saveRDS(cars_train, "cars_train.Rdata")
```

### 4.1.2) `Test` sample:

```{r message= FALSE, eval=FALSE}
cars_test <- cars[-cars_which_train,]
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE, eval=FALSE}
saveRDS(cars_test, "cars_test.Rdata")
```

## 4.2) For the modesl sensible to data distribution:

There are some models sensible to `data distribution` based on `euclidean distance`.

In our dataset there are many `factors` and this can be time consuming for some models, hence all these `factors` will be trasnformed to `numeric`:

```{r echo= TRUE, results = 'hide', message= FALSE, eval=FALSE}
cars2 <- cars[-26]
indx <- sapply(cars2, is.factor)
cars2[indx] <- lapply(cars2[indx], function(x) as.numeric(as.character(x)))
cars2 <- cbind(cars2, cars[26])
cars_preProces1 <-
  preProcess(cars2, method = c("range"))
cars2 <- predict(cars_preProces1,
               cars2)
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
cars_which_train1 <-
  createDataPartition(cars2$symboling,
                      p = 0.7, 
                      list = FALSE)
```

### 4.2.1) `Training` sample:

```{r message= FALSE, eval=FALSE}
cars_train1 <- cars2[cars_which_train1,]
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE, eval=FALSE}
saveRDS(cars_train1, "cars_train1.Rdata")
```

### 4.2.2) `Test` sample:

```{r message= FALSE, eval=FALSE}
cars_test1 <- cars2[-cars_which_train1,]
```

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE, eval=FALSE}
saveRDS(cars_test1, "cars_test1.Rdata1")
```

# 5) `Features selection`:

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE}
cars_train <- readRDS("cars_train.Rdata")
cars_test <- readRDS("cars_test.Rdata")
```

It will be applied only to `cars_train` sample:

## 5.1) `Correlation` between features:

We will check the correlation between all the `numerical variables`, so we will use the list `cars_numeric_vars`.

Firstly, the correlation will be checked by the graph `corplot`:

```{r message= FALSE, echo=FALSE, eval=FALSE}
cars_correlations <-
  cor(cars_train[, cars_numeric_vars],
      use = "pairwise.complete.obs")
saveRDS(cars_correlations, "cars_correlations.rds")
```

```{r message= FALSE, echo=FALSE}
cars_correlations <- readRDS("cars_correlations.rds")
corrplot.mixed(
  cars_correlations,
  upper = "square",
  lower = "number",
  tl.col = "black",
  tl.pos = "lt"
)
```

It seems that the variables are `not highly correlated`, there are not values close to dark blue or dark red.

To be sure, we will check the `maximum` and `minimum` correlations, but in the maximum the value one should be removed, because each variable is evaluated against itself:

```{r message= FALSE, echo= FALSE}
data.frame(Maximum.correlation = (max(cars_correlations[cars_correlations != 1]))) %>% 
  kable(align = "l",digits = 5) 
```
```{r message= FALSE, echo= FALSE}
data.frame(Minimum.correlation = (min(cars_correlations))) %>% 
   kable(align = "l",digits = 5) 
```

The maximum correlation was `0.17139`, and the minimum `-0.16221`, hence there is not any variable to be omitted.

## 5.2) `Relationship` with the target variable:

Now we will check if the characteristic variables have relationship with the target variable `symboling`, so we will use the list `cars_mult_bin_vars`.

In order to be able to check this, we will use `ANOVA`, with the created function `result_aov_pvalue`:

```{r message= FALSE, warning= FALSE, result= "hide"}
result_aov_pvalue <- function(data, var) {
  result <- c()
  for (i in var) {
    if (summary(aov(data[, 10] ~ data[, i]))[[1]][["Pr(>F)"]][1] < 0.05) {
      result <-
        c(result,
          paste("Reject H0 -", i, "impact in symboling"))
    }
    else {
      result <-
        c(result,
          paste("NO reject H0 -", i, "has not impact in symboling"))
    }
  }
  return(result)
}
```

We will apply the function to our data:

```{r message= FALSE, echo= FALSE}
data.frame(Decision = result_aov_pvalue(cars_train, cars_mult_bin_vars)) %>% 
  kable(align = "l",digits = 2) 
```

The `null hypothesis` means that the variable `has not impact` in the taget variable (symboling).

For all the cases we `reject` this `null hypothesis`. Hence, considering 5% as the level of significance, we can conclude that `all the characteristic variables have impact in the target variable`.

## 5.3) Variables with `near zero variance`:

The variables with zero or near zero variance can have `negative impact` on the final result of the applied algorithm, for this reason is important to be checked.

The function `nearZeroVar` from the `caret package` will be used:

```{r message= FALSE, eval=FALSE}
cars_nzv_stats <- nearZeroVar(cars_train,
                              saveMetrics = TRUE)
saveRDS(cars_nzv_stats, "cars_nzv_stats.rds")
```

```{r message= FALSE, results = 'hide', message= FALSE}
cars_nzv_stats <- readRDS("cars_nzv_stats.rds")
cars_nzv_stats_res <- cars_nzv_stats %>%
  rownames_to_column("variable") %>%
  arrange(-zeroVar, -nzv, -freqRatio)
cars_nzv_stats_res[c(1, 4:5)] %>% 
kable(align = "l",digits = 2)
```

```{r message= FALSE, results='asis'}
cars_nzv_unsel <- c(cars_nzv_stats_res[1][cars_nzv_stats_res[5] == TRUE])
sort(cars_nzv_unsel) %>% 
  knitr::knit_print()
```

As we can see there is not any variable with `TRUE` for `nzv` and `zeroVar`, so the model does not suggest to omit any feature.

It can be concluded that no variables should be omitted considering near zero variance method.

## 5.4) `Linear regressions` in dataset:

We will check if in the dataset `cars`there is linear regression with the function `findLinearCombos`:

```{r message= FALSE, results='asis'}
cars_linearCombos <- findLinearCombos(cars_train[, cars_numeric_vars])  %>% 
  knitr::knit_print()
```

The function returns `NULL`, so there is not linear regression in our data.

## 5.5) `Rank features` - Learning Vector Quantization:

`Learning Vector Quantization` will be applied in order to rank the features by importance in relationship with `symboling`.

This algorithm is being applied with `down-sampling` and without `cross-validation`, and it can give an idea of which feauture could be omitted.

```{r message= FALSE, eval=FALSE}
set.seed(16)
ctrl_cvnone <- trainControl(method = "none",
                            sampling = "down")
rank_features <-
  train(
    symboling ~ .,
    data = cars_train,
    method = "lvq",
    trControl = ctrl_cvnone
  )
saveRDS(rank_features, "rank_features.rds")
```

```{r message= FALSE, echo= FALSE, results='asis'}
rank_features <- readRDS("rank_features.rds")
rank_features %>% 
knitr::knit_print()
```

```{r message= FALSE, echo= FALSE}
importance_rank_features <- varImp(rank_features, scale = FALSE)
plot(importance_rank_features,
      col = "#9F1E42")
```

* `Most important` feautures by level:
  + `neutral`: `engine.type`
  + `risky`: `num.of.doors`
  + `secure`: `engine.type`
  
* `Less important` feautures by level:
  + `neutral`: `compression.ratio`
  + `risky`: `compression.ratio`
  + `secure`: `compression.ratio`

As we can see, the feature `engine.type` is one of the most important for all the levels. 

On the other hand, in case we need to ommit some feautures, probably we will choose `compression.ratio`, as it is the least important for all the levels.

## 5.6) `Conclusions`:

The previous analysis did not suggest to ommit any feauture, but the variable `make` has too many levels. Cosequently, it can be a problem in terms of `computational time` when training the different models.

For this reason, in the less demanding models will be tested whether omitting the feature `make` gives much worst results.

# 6) Application of `Classification algorithms`:

## 6.1) `Functions`:

Two functions were created in order to evaluate the results of the models:

### 6.1.1) `Accuracy` - accuracy_multinom:

The first one `accuracy_multinom` returns the accuracy measures to compare the performance of the models:

```{r message= FALSE, warning= FALSE, result= "hide"}
accuracy_multinom <- function(predicted, real) {
  ctable_m <- table(predicted,
                    real)
  accuracy <- (100 * sum(diag(ctable_m)) / sum(ctable_m))
  base_ <- diag(ctable_m) / colSums(ctable_m)
  balanced_accuracy <- mean(100 * ifelse(is.na(base_), 0, base_))
  base_2 <- diag(ctable_m) / rowSums(ctable_m)
  correctly_predicted <-
    mean(100 * ifelse(is.na(base_2), 0, base_2))
  return(
    data.frame(
      accuracy = accuracy,
      balanced_accuracy = balanced_accuracy,
      balanced_correctly_predicted = correctly_predicted
    )
  )
}
```

### 6.1.1) `Graph` - plot_model_fitted:

The function `plot_model_fitted` returns a `ggplot` graph of the prediction results, in order to see how the predictors are divided by levels:

```{r message= FALSE, warning= FALSE, result= "hide"}
plot_model_fitted <- function(fitt) {
  require(scales)
  a <- data.frame(Symboling = fitt)
  ggplot(a, aes(Symboling)) +
    geom_bar(fill = "#9F1E42") +
    theme_minimal() +
    scale_y_continuous(labels = comma) 
    
}
```

## 6.2) `MLR` - Multinomial Logistic Regression:

The first model to be applied will be `multinomial logistic regression`. It is a classification method that generalize logistic regression to multiclass problems, in this case three classes or levels.

### 6.2.1) Train the data:

The first step in the application of one classisication method is to `train` the model. To do that, the algorithm should be applied to the training sample, in this case `cars_train`.

The level of maximum iterations will be selected as 1000, because the predeterminate in 100, and the algorithm can converge between both number of iterations.

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial <- multinom(symboling ~ .,
                    data = cars_train,
                      maxit = 1000)
saveRDS(mlr_multinomial, "mlr_multinomial.rds")
```

```{r message= FALSE, echo=FALSE}
mlr_multinomial <- readRDS("mlr_multinomial.rds")
data.frame(Residual.Deviance = round(mlr_multinomial[["deviance"]], 2), AIC = round(mlr_multinomial[["AIC"]], 2)) %>% 
  kable(align = "l",digits = 2) 
```

```{r message= FALSE, echo=FALSE}
mlr_multinomial <- readRDS("mlr_multinomial.rds")
data.frame(Residual.Deviance = round(mlr_multinomial[["deviance"]], 2), AIC = round(mlr_multinomial[["AIC"]], 2)) %>% 
  kable(align = "l",digits = 2) 
```

### 6.2.2) Prediction on test sample:

After the model is `trained`, we are reade to make the prediction on the `test` sample:

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial_fitted <- predict(mlr_multinomial,
                              cars_test)
saveRDS(mlr_multinomial_fitted, "mlr_multinomial_fitted.rds")
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial_fitted_prob <- predict(mlr_multinomial,
                              cars_test,
                              type= "prob")
saveRDS(mlr_multinomial_fitted_prob, "mlr_multinomial_fitted_prob.rds")
```

### 6.2.3) Results:

First of all, we will see how the predictions are divided by levels, with the help of the function `plot_model_fitted`, also we will see a table with the distribution:

```{r message= FALSE, echo=FALSE}
mlr_multinomial_fitted <- readRDS("mlr_multinomial_fitted.rds")
plot_model_fitted(mlr_multinomial_fitted)
```
```{r message= FALSE, echo=FALSE}
table(mlr_multinomial_fitted) %>% 
  kable(align = "l",digits = 2) 
```

As we can see in the above graph and table, there are not a lot of levels predicted with the level `secure`, this is one of the consequences of working with `imbalanced` data.

Most of the cars were predicted as `risky`, 228.513 cars.

Below we can find a table in order to compare the result on the test sample:

```{r message= FALSE, echo=FALSE}
(ctable_mlr_multinomial <- table(mlr_multinomial_fitted,
                       cars_test$symboling)) %>% 
  kable(align = "l",digits = 2)
```

In the table we can see that the `risky` level falls from 228.513 to 141.773. This is because only 141.773 `risky` cars were properly predicted, and the rest of the cars have other levels, 57.423 `neutral` and 29.317 `secure`.

Finally, we will apply the function `accuracy_multinom` in order to see the different measures of `accuracy` of our model:

```{r message= FALSE, echo=FALSE}
accuracy_multinom(predicted = mlr_multinomial_fitted,
                  real = cars_test$symboling) %>% 
  kable(align = "l",digits = 6)
```

Below there is the `ROC measure`:

```{r message= FALSE, echo=FALSE}
mlr_multinomial_fitted_prob <- readRDS("mlr_multinomial_fitted_prob.rds")
data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, mlr_multinomial_fitted_prob)[6])) %>% 
  kable(align = "l",digits = 5)
```

The `accuracy` is not really high, however the `balance accuracy` is penalized by the fact of having an `imbalanced` data.

* We will see if the accuracy is reduced a lot when considering the feature `make`:

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial_sel <- multinom(symboling ~ .,
                    data = cars_train %>% 
                      dplyr::select(-make),
                      maxit = 1000)
saveRDS(mlr_multinomial_sel, "mlr_multinomial_sel.rds")
```

```{r message= FALSE, results='asis'}
mlr_multinomial_sel <- readRDS("mlr_multinomial_sel.rds")
data.frame(Residual.Deviance = round(mlr_multinomial_sel[["deviance"]], 2), AIC = 
             round(mlr_multinomial_sel[["AIC"]], 2)) %>%
  kable(align = "l",digits = 2) 
```

As we can see `AIC` and `residual deviance` increases in comparison with the model with `make`.

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial_sel_fitted <- predict(mlr_multinomial_sel,
                              cars_test)
saveRDS(mlr_multinomial_sel_fitted, "mlr_multinomial_sel_fitted.rds")
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
mlr_multinomial_sel_fitted_prob <- predict(mlr_multinomial_sel,
                              cars_test,
                              type= "prob")
saveRDS(mlr_multinomial_sel_fitted_prob, "mlr_multinomial_sel_fitted_prob.rds")
```

```{r message= FALSE, echo=FALSE}
mlr_multinomial_sel_fitted <- readRDS("mlr_multinomial_sel_fitted.rds")
accuracy_multinom(predicted = mlr_multinomial_sel_fitted,
                  real = cars_test$symboling) %>% 
  kable(align = "l",digits = 6)
```

```{r message= FALSE, echo=FALSE}
mlr_multinomial_sel_fitted_prob <- readRDS("mlr_multinomial_sel_fitted_prob.rds")
data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, mlr_multinomial_sel_fitted_prob)[6])) %>% 
  kable(align = "l",digits = 5)
```

As we can see, the `accuracy` was reduced but not significantly, the same occurs with `ROC`.

We can conclude that removing the feature `make` has not a lot of impact in the final results of the models.

## 6.3) `PMR` - Penalized Multinomial Regression:

`Caret package` brings several models, one of them is `Penalized Multinomial Regression`. It is the closest one to the normal multinomial model.

The benefits of applying this algorithm with `caret package` are that we will be able to apply `resampling` and `cross-validation`. Adittionally, as the previous normal `multinomial` model, we will apply 1.000 as the maximum number of iterations.

For this model we will apply `cross-validation` with `5 fold`:

### 6.3.1) Train the data:

```{r message= FALSE, eval=FALSE}
set.seed(16)

ctrl_cv <- trainControl(method = "cv",
                          number = 5)
options(scipen=999)
pmr_multinomial <- train(
  symboling ~ .,
  data = cars_train,
  method = "multinom",
  trControl = ctrl_cv,
  maxit = 1000
)
saveRDS(pmr_multinomial, "pmr_multinomial.rds")
```

```{r message= FALSE, echo= FALSE, results='asis'}
pmr_multinomial <- readRDS("pmr_multinomial.rds")
pmr_multinomial %>% 
knitr::knit_print()
```

### 6.3.2) Prediction on test sample:

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_fitted <- predict(pmr_multinomial,
                              cars_test)
saveRDS(pmr_multinomial_fitted, "pmr_multinomial_fitted.rds")
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_fitted_prob <- predict(pmr_multinomial,
                              cars_test,
                              type= "prob")
saveRDS(pmr_multinomial_fitted_prob, "pmr_multinomial_fitted_prob.rds")
```

### 6.3.3) Results:

Below we can find the optimal model values:

```{r message= FALSE, echo=FALSE, results='asis'}
pmr_multinomial_fitted <- readRDS("pmr_multinomial_fitted.rds")
data.frame(Residual.Deviance = round(pmr_multinomial$finalModel[["deviance"]], 2), AIC = round(pmr_multinomial$finalModel[["AIC"]], 2)) %>% 
  kable(align = "l",digits = 2) 
```

The `AIC` and `residual deviance` values obtained are lower than the normal `multinomial` model.

It seems that the optimal `decay` value that increases more the accuacy is 0.0001

```{r message= FALSE, echo=FALSE}
plot_model_fitted(pmr_multinomial_fitted)
```
```{r message= FALSE, echo=FALSE}
table(pmr_multinomial_fitted) %>% 
  kable(align = "l",digits = 2) 
```

As we can see in the above graph and table, there are not a lot of levels predicted with the level `risky`, this is one of the consequences of working with unbalanced data.

Below we can find a table in order to compare the result on the test sample:

```{r message= FALSE, echo=FALSE}
(ctable_pmr_multinomial <- table(pmr_multinomial_fitted,
                       cars_test$symboling)) %>% 
  kable(align = "l",digits = 2)
```

In the table we can see that the `risky` level falls from 228.512 to 141.769. This is because only 141.769 `risky` cars were properly predicted, and the rest of the cars have other level, 57.425 `neutral` and 29.318 `secure`.

Finally, we will apply the function `accuracy_multinom` in order to see the different measures of `accuracy` of our model:

```{r message= FALSE, echo=FALSE}
accuracy_multinom(predicted = pmr_multinomial_fitted,
                  real = cars_test$symboling) %>% 
  kable(align = "l",digits = 6)
```

```{r message= FALSE, echo=FALSE}
pmr_multinomial_fitted_prob <- readRDS("pmr_multinomial_fitted_prob.rds")
data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_fitted_prob)[6])) %>% 
  kable(align = "l",digits = 5)
```

As we can see all the accuracies increased, but not much more than the first model, also `ROC` increases.

## 6.4) `Test` - Choosing the kind of sampling:

As we saw the `multinom` model of caret package is not really demanding in terms of `computational time`, so we will use this model to test which `sampling` method should be applied in `KNN` and `SVM`.

We will see if the accuracy changes if we apply `no-sampling` and `down-sampling`.

### 6.4.1) `No sampling`:

```{r message= FALSE, eval=FALSE}
set.seed(16)

ctrl_cv <- trainControl(method = "cv",
                        number = 2)
options(scipen=999)
pmr_multinomial_nosam <- train(
  symboling ~ .,
  data = cars_train,
  method = "multinom",
  trControl = ctrl_cv,
  maxit = 1000
)
saveRDS(pmr_multinomial_nosam, "pmr_multinomial_nosam.rds")
```

```{r message= FALSE, echo= FALSE, results='asis'}
pmr_multinomial_nosam <- readRDS("pmr_multinomial_nosam.rds")
pmr_multinomial_nosam %>%  
knitr::knit_print()
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_nosam_fitted <- predict(pmr_multinomial_nosam,
                              cars_test)
saveRDS(pmr_multinomial_nosam_fitted, "pmr_multinomial_nosam_fitted.rds")
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_nosam_fitted_prob <- predict(pmr_multinomial_nosam,
                              cars_test,
                              type= "prob")
saveRDS(pmr_multinomial_nosam_fitted_prob, "pmr_multinomial_nosam_fitted_prob.rds")
```

Below we can find the predicted results by `class`:

```{r message= FALSE, echo=FALSE}
pmr_multinomial_nosam_fitted <- readRDS("pmr_multinomial_nosam_fitted.rds")
plot_model_fitted(pmr_multinomial_nosam_fitted)
```

```{r message= FALSE, echo=FALSE}
accuracy_multinom(predicted = pmr_multinomial_nosam_fitted,
                  real = cars_test$symboling) %>% 
  kable(align = "l",digits = 6)
```

```{r message= FALSE, echo=FALSE}
pmr_multinomial_nosam_fitted_prob <- readRDS("pmr_multinomial_nosam_fitted_prob.rds")
data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_nosam_fitted_prob)[6])) %>% 
  kable(align = "l",digits = 5)
```

### 6.4.2) `Down-sampling`:

```{r message= FALSE, eval=FALSE}
set.seed(16)
ctrl_cv <- trainControl(method = "cv",
                          number = 2,
                          sampling = "down",
                        classProbs = TRUE,
                        summaryFunction = fiveStats)
options(scipen=999)
pmr_multinomial_down <- train(
  symboling ~ .,
  data = cars_train,
  method = "multinom",
  trControl = ctrl_cv,
  maxit = 1000
)
saveRDS(pmr_multinomial_down, "pmr_multinomial_down.rds")
```

```{r message= FALSE, echo= FALSE, results='asis'}
pmr_multinomial_down <- readRDS("pmr_multinomial_down.rds")
pmr_multinomial_down %>% 
knitr::knit_print()
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_down_fitted <- predict(pmr_multinomial_down,
                              cars_test)
saveRDS(pmr_multinomial_down_fitted, "pmr_multinomial_down_fitted.rds")
```

```{r message= FALSE, eval=FALSE}
set.seed(16)
pmr_multinomial_down_fitted_prob <- predict(pmr_multinomial_down,
                              cars_test,
                              type= "prob")
saveRDS(pmr_multinomial_down_fitted_prob, "pmr_multinomial_down_fitted_prob.rds")
```

Below we can find the predicted results by `class`:

```{r message= FALSE, echo=FALSE}
pmr_multinomial_down_fitted <- readRDS("pmr_multinomial_down_fitted.rds")
plot_model_fitted(pmr_multinomial_down_fitted)
```

As we can find in the above graph, the predicted classes are not `imbalanced`, hence `down-sampling` will deal with this problem.

```{r message= FALSE, echo=FALSE}
accuracy_multinom(predicted = pmr_multinomial_down_fitted,
                  real = cars_test$symboling) %>% 
  kable(align = "l",digits = 6)
```

```{r message= FALSE, echo=FALSE}
pmr_multinomial_down_fitted_prob <- readRDS("pmr_multinomial_down_fitted_prob.rds")
data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_down_fitted_prob)[6])) %>% 
  kable(align = "l",digits = 5)
```

### 6.4.4) Conclusions of the `test`:

The above results are typical when one has `imbalanced` data. In this case one cannot trust the `accuracy`, because the predictor tends to predict the class that has more observations. 

We can see that the `accuracy` is greater for no-sampling, but the `ROC curve measure` is greater for `down-sampling`, hence it would be better the last model with `down-sampling`.

Applying `down-sampling` is really efficent, we obtain better predictor, and also we reduce the computational time, and this is important in the case of big samples like this.

Probably applying `ROSE` or `SMOTE` sampling would give even better results, but `computational time` probably would increase drastically.

On the otehr hand, it is not productive to use the feature `make` which contains 22 levels, because the computational time increases a lot, but probably it would not be a lot of difference in terms of `performance`, as we saw before in `multinomial`.

We can conclude that `down-sampling` willl be applied in the next algorithms, also the feature `make` will be omitted.

## 6.5) `KNN` - K-Nearest Neighbors: 

```{r echo=FALSE, echo = F, results = 'hide', message= FALSE}
cars_train1 <- readRDS("cars_train1.Rdata")
cars_test1 <- readRDS("cars_test1.Rdata")
```

`k-nearest neighbors algorithm` is one of the most famous classification methods. In this case the input consists of the k closest training samples in the feature space. 

It uses `euclidean distance` in order to measure the distance between neighbors, for this reason, for this algorithm is important to have scaled data, in our case the data was scaled before, with the `range method`, in the range [0, 1]. 

The algorithm will be applied with `cross-validation`, `3 folds`, and `down-sampling`. 

Adittionally, we will apply `tuneGrid`, in order to personalize more parameters, in this case several values of `k` are going to be tested, corresponding to the sequence `seq(5, 89, 14)))`. Hence, 18 values will be tested 5, 19.. till 80. The algorithm will return the `finalmodel` with the greatest `accuracy`.

### 6.5.1) Train the data:

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 k_value <- data.frame(k = c(seq(5, 145, 28)))
 ctrl_cv3 <- trainControl(method= "cv",
                             number= 3,
                          sampling= "down") 
 knn_model <- train(symboling ~ ., 
                      cars_train1 %>% 
                      dplyr::select(-make), 
                      method = "knn", 
                      trControl =  ctrl_cv3,
                      tuneGrid = k_value) 
 saveRDS(knn_model, "knn_model.rds") 
```
 
```{r message= FALSE, echo= FALSE, results='asis'}
knn_model <- readRDS("knn_model.rds")
knn_model %>% 
knitr::knit_print()
```
 
### 6.5.2) Prediction on test sample: 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 knn_model_fitted <- predict(knn_model, 
                               cars_test1) 
 saveRDS(knn_model_fitted, "knn_model_fitted.rds") 
``` 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 knn_model_fitted_prob <- predict(knn_model, 
                               cars_test1, 
                               type= "prob") s
 saveRDS(knn_model_fitted_prob, "knn_model_fitted_prob.rds") 
``` 

### 6.5.3) Results: 

```{r message= FALSE, echo= FALSE, result= "hide"} 
knn_model_fitted <- readRDS("knn_model_fitted.rds")
knn_model_fitted_prob <- readRDS("knn_model_fitted_prob.rds")
``` 

Below we can find the optimal `k` value for the given sequence:

```{r message= FALSE, results='asis'} 
 knn_model$finalModel$k %>% 
  knitr::knit_print()
``` 

We can plot the results of the `cross-validation`: 

```{r message= FALSE, echo= FALSE} 
 plot(knn_model, 
      col = "#9F1E42") 
``` 

```{r message= FALSE, echo=FALSE} 
 plot_model_fitted(knn_model_fitted) 
``` 
```{r message= FALSE, echo=FALSE} 
 table(knn_model_fitted) %>% 
   kable(align = "l",digits = 2) 
``` 

Most of the cars were predicted as `risky`, 192.140 cars. 

Below we can find a table in order to compare the result on the test sample: 

```{r message= FALSE, echo=FALSE} 
 (ctable_knn_model_fitted <- table(knn_model_fitted, 
                        cars_test1$symboling)) %>% 
   kable(align = "l",digits = 2) 
``` 

 Finally, we will apply the function `accuracy_multinom` in order to see the different measures of `accuracy` of our model: 

```{r message= FALSE, echo=FALSE} 
 accuracy_multinom(predicted = knn_model_fitted, 
                   real = cars_test1$symboling) %>%
  kable(align = "l",digits = 6) 
``` 

```{r message= FALSE, echo=FALSE} 
 knn_model_fitted_prob <- readRDS("knn_model_fitted_prob.rds") 
 data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test1$symboling, knn_model_fitted_prob)[6])) %>%
  kable(align = "l",digits = 5)
``` 

The `accuracy` and `ROC` measure is much worse that the previous cases. 

Probably this algorithm is not the most adequate one to be applied when the dependent variable has multiclasses.

* `Support vector algoprithm` is a more complex algorithm. There are several types, and the `linear` creates a line or hyperplane which separates the data into classes. There are other kinds of `SVM` algorithm, such as `polynomial` and `radial`, these two are more complex in terms of `computational time`, hence it is not a good idea to apply them in a large dataset. 

During this analysis `SVM` was applied, however the model was not trained after two days of processing, hence the processes was stopped and will not be presented here.

## 6.6) `LDA` - Linear Discriminant Analysis: 

Now `Linear discriminant analysis` will be applied. This algorithm is popular when dealing with a multiclass dependent variable, like our variable `symboling`.

It finds a linear combination of features that characterizes or separates two or more classes of objects.

In this case it will be applied with `repeat cross-validation`, `10 folds`, `repeated 3 times` and `down sampling` In this case as it is not based on `euclidean distance`, it can be applied to `train_cars` instead of `train_cars1`. Aditionally, the feature `make` will not be omitted, because this model can manage very well the `ordinal` variables with several levels.

### 6.6.1) Train the data 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 ctrl_cv10 <- trainControl(method= "repeatedcv", 
                           number = 10, 
                           repeats = 3,
                           sampling= "down") 
lda_model <- train( 
   symboling ~ ., 
   data = cars_train, 
   method = "lda", 
   trControl = ctrl_cv10) 
 saveRDS(lda_model , "lda_model.rds") 
``` 

```{r message= FALSE, echo= FALSE, results='asis'}
lda_model <- readRDS("lda_model.rds")
lda_model %>% 
knitr::knit_print()
```

### 6.6.2) Prediction on test sample 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 lda_model_fitted <- predict(lda_model, 
                               cars_test) 
 saveRDS(lda_model_fitted, "lda_model_fitted.rds") 
``` 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 lda_model_fitted_prob <- predict(lda_model, 
                               cars_test, 
                               type= "prob") 
 saveRDS(lda_model_fitted_prob, "lda_model_fitted_prob.rds") 
``` 

### 6.6.3) Results 

```{r message= FALSE, echo= FALSE, result= "hide"} 
lda_model_fitted <- readRDS("lda_model_fitted.rds")
lda_model_fitted_prob <- readRDS("lda_model_fitted_prob.rds")
``` 

```{r message= FALSE, echo=FALSE} 
 plot_model_fitted(lda_model_fitted) 
``` 
```{r message= FALSE, echo=FALSE} 
 table(lda_model_fitted) %>% 
   kable(align = "l",digits = 2) 
``` 

 As we can see in the above graph and table, there are not a lot of levels predicted with the level `secure`, this is one of the consequences of working with `imbalanced` data. 

 Most of the cars were predicted as `risky`, 113.509 cars. 

 Below we can find a table in order to compare the result on the test sample: 

```{r message= FALSE, echo=FALSE} 
 (ctable_lda_model <- table(lda_model_fitted, 
                        cars_test$symboling)) %>% 
   kable(align = "l",digits = 2) 
``` 

Finally, we will apply the function `accuracy_multinom` in order to see the different measures of `accuracy` of our model: 

```{r message= FALSE, echo=FALSE} 
 accuracy_multinom(predicted = lda_model_fitted, 
                   real = cars_test$symboling) %>%
  kable(align = "l",digits = 6) 
``` 

```{r message= FALSE, echo=FALSE, results='asis'} 
 lda_model_fitted_prob <- readRDS("lda_model_fitted_prob.rds") 
 data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, lda_model_fitted_prob)[6])) %>%
  kable(align = "l",digits = 5) 
``` 

The `ROC` obtained is similar to the `Multinomial Logistic Regression`.

## 6.7) `QDA` - Quadratic Discriminant Analysis: 

Similarly to the case of `SVM` there are other similar algorithms that apply other kind of combination rather than `linear`, for example `quadratic`, this last will be applied in this case.

In this case it will be applied with `repeat cross-validation`, `10 folds`, `repeated 3 times` and `down sampling`, in this case as it is not based on `euclidean distance`, it can be applied to `train_cars` instead of `train_cars1`. Aditionally, the feature `make` will not be omitted, because this model can manage well the `ordinal` variables with several levels.

### 6.7.1) Train the data 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 ctrl_cv10 <- trainControl(method= "repeatedcv", 
                           number = 10, 
                           repeats = 3,
                           sampling= "down") 
qda_model <- train( 
   symboling ~ ., 
   data = cars_train, 
   method = "qda", 
   trControl = ctrl_cv10) 
 saveRDS(qda_model , "qda_model.rds") 
``` 

```{r message= FALSE, echo= FALSE, results='asis'}
qda_model <- readRDS("qda_model.rds")
qda_model %>% 
knitr::knit_print()
```

### 6.6.2) Prediction on test sample 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 qda_model_fitted <- predict(qda_model, 
                               cars_test) 
 saveRDS(qda_model_fitted, "qda_model_fitted.rds") 
``` 

```{r message= FALSE, eval=FALSE} 
 set.seed(16) 
 qda_model_fitted_prob <- predict(qda_model, 
                               cars_test, 
                               type= "prob") 
 saveRDS(qda_model_fitted_prob, "qda_model_fitted_prob.rds") 
``` 

### 6.6.3) Results 

```{r message= FALSE, echo= FALSE, result= "hide"} 
qda_model_fitted <- readRDS("qda_model_fitted.rds")
qda_model_fitted_prob <- readRDS("qda_model_fitted_prob.rds")
``` 

```{r message= FALSE, echo=FALSE} 
 plot_model_fitted(qda_model_fitted) 
``` 
```{r message= FALSE, echo=FALSE} 
 table(qda_model_fitted) %>% 
   kable(align = "l",digits = 2) 
``` 

As we can see in the above graph and table, there are not a lot of levels predicted with the level `secure`, this is one of the consequences of working with `imbalanced` data. 

Most of the cars were predicted as `risky`, 127.163 cars. 

Below we can find a table in order to compare the result on the test sample: 

```{r message= FALSE, echo=FALSE} 
 (ctable_qda_model <- table(qda_model_fitted, 
                        cars_test$symboling)) %>% 
   kable(align = "l",digits = 2) 
``` 

Finally, we will apply the function `accuracy_multinom` in order to see the different measures of `accuracy` of our model: 

```{r message= FALSE, echo=FALSE} 
 accuracy_multinom(predicted = qda_model_fitted, 
                   real = cars_test$symboling) %>%
  kable(align = "l",digits = 6)  
``` 

```{r message= FALSE, echo=FALSE, results='asis'} 
 qda_model_fitted_prob <- readRDS("qda_model_fitted_prob.rds") 
 data.frame(Multiclass.ROC = as.numeric(multiclass.roc(cars_test$symboling, qda_model_fitted_prob)[6])) %>%
  kable(align = "l",digits = 5) 
``` 

The `ROC meausure` is the largest one, so this is one of the best models, as we cannot trust the `accuracy`

# 7) `Summary` and `conclusions`:

## 7.1) `Summary`:

```{r message= FALSE, echo=FALSE, eval= FALSE}
Model.name <- data.frame(c("MLR", "MLR -make", "PMR", "PMR no sam", "PMR sam", "KNN", "LDA", "QDA") )
Accuracy <- rbind(accuracy_multinom(predicted = mlr_multinomial_fitted,
                  real = cars_test$symboling),
                  accuracy_multinom(predicted = mlr_multinomial_sel_fitted,
                  real = cars_test$symboling),
                  accuracy_multinom(predicted = pmr_multinomial_fitted,
                  real = cars_test$symboling),
                  accuracy_multinom(predicted = pmr_multinomial_nosam_fitted,
                  real = cars_test$symboling),
                  accuracy_multinom(predicted = pmr_multinomial_down_fitted,
                  real = cars_test$symboling),
                  accuracy_multinom(predicted = knn_model_fitted, 
                   real = cars_test1$symboling),
                  accuracy_multinom(predicted = lda_model_fitted, 
                   real = cars_test$symboling),
                  accuracy_multinom(predicted = qda_model_fitted, 
                   real = cars_test$symboling)
                  )
ROC <- rbind(as.numeric(multiclass.roc(cars_test$symboling, mlr_multinomial_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, mlr_multinomial_sel_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_nosam_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, pmr_multinomial_down_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test1$symboling, knn_model_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, lda_model_fitted_prob)[6]),
            as.numeric(multiclass.roc(cars_test$symboling, qda_model_fitted_prob)[6])
                  ) %>% data.frame()
Var_selection <- data.frame(c("No", "Yes", "No", "No", "No", "Yes", "No", "No"))
CV <- data.frame(c("No", "No", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes"))
Folds <- data.frame(c("0", "0", "5", "2", "2", "3", "10", "10"))
Resampling <- data.frame(c("No", "No", "No", "No", "Down", "Down", "Down", "Down"))
summary <- cbind(Model.name, Accuracy, ROC, Var_selection, CV, Folds, Resampling)
colnames(summary) <- c("Model name", "Accuracy", "Bal. accuracy", "Bal. correct. accuracy", "ROC", "Var. select.", "CV", "Fold", "Resampling")
saveRDS(summary, "summary.Rds")
```

```{r message= FALSE, echo=FALSE}
summary <- readRDS("summary.Rds")
summary %>% 
  kable(align = "l",digits = 2)
```

## 7.2) `Conclusions`:

* For the dataset `cars` the best `classification` model is `Quadratic Discriminant` with ` repeated cross validation` and `down-sampling`
* In `imbalanced` data one cannot trust the `accuracy` to compare models, the model tends to predict the most common `class`, hence other kind of measures are recomended, in our case `ROC` was used
* `K nearest neighbors` and `support vector machine` are models that do not work very well with `multiclass` dependent variable, it would be better to use them in case of `binomial` depdent varaible
* Before applying demanding models, it is convenient to analize `ceteris paribus` what is expected to happen, in our case it was analized what will happen when `down-sampling` is applied
* `Computational time` matters, and it should be taken into consideration when applying `machine learning` models, hence the `data preparation` is really important in these cases. In certain instances it is recommended to `parallel` the process, when the algorithm allows to do it (`doParallel` and `doMC` packages can be used)
* In case of having a `big data` and `imbalance clases` it is really productive to use `down-sampling`, in terms of `computational time` and `performance` of the model
* When performing a `Machine Learning` analysis it is important to have enough memory in the system which would allow one to save the results

# 8) `References`:

* Class materials provided by Piotr Wjcik PhD at the course Machine Learning 1, University of Warsaw, 2020
* Photo source: https://unsplash.com/photos/FkJ3aNGeFMY
* https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
* https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/
* http://topepo.github.io/caret/index.html
* https://machinelearningmastery.com/compare-the-performance-of-machine-learning-algorithms-in-r/
